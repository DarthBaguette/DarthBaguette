\documentclass[../PhysicsFormulae.tex]{subfiles}
\begin{document}

\subsection{Types of Error}
Error is generally formatted in two equivalent ways:
\[ G = 6.67430(15) \times 10^{-11} \; \mathrm{\frac{m^3}{kg \cdot s^2}} \]
\[ G = 6.67430 \pm 0.00015 \times 10^{-11} \; \mathrm{\frac{m^3}{kg \cdot s^2}} \]
When conducting experiments, a rule of thumb is to keep 1 significant figure on the errors and round the measurement to the same decimal place. \bigskip 

Even with careful measurement, error can occur for different reasons, such as
\begin{enumerate}
    \itemsep0em
    \item Accuracy of tool: ruler only gives $\pm 0.1 cm$
    \item Problem of definition: measuring 'height' is difficult because it is not constant throughout the day
\end{enumerate}
Broadly, there are thus two types of errors:

\textbf{Random Error}: small, unobserved errors---e.g., a measurement is slightly different than what you see
\begin{itemize}
    \item Independent random variables are special in that increasing the accuracy of one measurement does not affect the others. 
\end{itemize}
\textbf{Systematic Error}: calibration errors (oftentimes)---e.g., your metal ruler shrinks on a cold day

\subsection{Estimating Error}
Devices that deliver measurements are generally either
\begin{enumerate}
    \itemsep0em
    \item Digital, where a numerical value is outputted (such as a scale in Q's chemistry class)---for these, the error is the nearest decimal place
    \item Analog, where some indicator stops between markings (such as a scale in RN)---for these, the error is half the distance between markings
\end{enumerate}

\subsection{Variance}
Suppose you took measurements of time that lasted between 8 and 10 seconds. It'd be probably safe to call that $9 \pm 1$ seconds, but how can we be more precise? This is where \textbf{residuals} are useful, defined as the difference between a value and the mean. 
\[ r = x_i - \mathrm{<}x\mathrm{>} \]
where $\mathrm{<}x\mathrm{>}$ denotes the But if we average the residuals, by definition it must be 0---that's not good! So we introduce \textbf{variance}, defined as the average of the \textit{squares} of the residuals. Averaging the absolute value of the residuals would theoretically work, but variance has nicer mathematical properties. 
\[ \mathrm{var}(x) = \mathrm{<}(x - \mathrm{<}x\mathrm{>})^2\mathrm{>} \]
By expanding the expression, it can be shown that 
\[ \mathrm{var}(x) = \mathrm{<}x^2\mathrm{>} - \mathrm{<}x\mathrm{>}^2 \]

\subsection{Error Propagation}
\subsubsection{Systematic Error}
For independent systematic errors, simply add up the errors. This is because the directionality of the error should be identical; for instance, if your ruler is short by 1 inch, all the errors will add up in the same way, none of them canceling out. 

\subsection{Independent Errors}
Independent variables aren't effected by each other, so
\[ P(x|y) = P(x) \; \& \; P(y|x) = P(y) \]
And in general, the probability of two events (variables) is 
\[ P(x \cap y) = P(x) P(y|x) = P(y) P(x|y) \]
So it follows that for independent variables, 
\[ P(x \cap y) = P(x)P(y) \]
The average value of the product of both variables is thus
\[ \mathrm{<}xy\mathrm{>} = \int_{xy} xyP(x \cap y) \; d(xy) = \int xP(x) \; dx \int yP(y) \; dy = \mathrm{<}x\mathrm{>} \mathrm{<}y\mathrm{>} \]

\subsubsection{Sums and Differences}
Error is defined through standard deviations, so let's consider the variance:
\[ \mathrm{var}(x + y) = \mathrm{<}(x + y)^2 \mathrm{>} - \mathrm{<}x + y\mathrm{>}^2 = \mathrm{var}(x) + \mathrm{var}(y) \]
arriving at the final expression after some algebra. Rewriting, we have 
\[ \sigma_{x+y}^2 = \sigma_x^2 + \sigma_y^2 \]
Thus, errors (standard deviation) add in \textbf{quadrature}. More formally, if $Q$ is the sum or difference of several measurements such that \[Q = \left(a+b+...+c\right) - \left(x+y+...+z\right)\]
then 
\[\delta Q = \sqrt{[\left(\delta a\right)^2 + \left(\delta b\right)^2 +...+\left(\delta c\right)^2] + [\left(\delta x\right)^2 + \left(\delta y\right)^2 +...+ \left(\delta z\right)^2]}\]
Note that $\delta$ and $\sigma$ are interchangeable and equivalent. \bigskip

Suppose we take $N$ measurements, each with error $\delta x$. Then the overall error becomes 
\[ \delta (Nx) = \sqrt{N} \delta x \rightarrow \delta(\mathrm{<}x\mathrm{>}) = \frac{\delta (Nx)}{N} = \frac{\delta x}{\sqrt{N}} \]
This is known as the $1/\sqrt{N}$ rule---the error goes inversely with the number of trials/measurements you perform. This rule is also used in radioactive decay; for every $N$ decaying particles observed, the error is $\sqrt{N}$. 

\bigskip
Suppose you have several measurements of the same parameter but by different tools, so all the errors are different. How should you average the measurements to minimize the error? Choose the weights so that they are inversely proportional to the squares of the error. For instance, if a ruler has 1/2 the uncertainty of a measuring rod, weigh the ruler 4 times as much in the weighted average. 

\subsubsection{Products and Quotients}
While the absolute error adds in quadrature when measurements are summed, the \textit{relative} error adds in quadrature when measurements are multiplied. If $Q$ is the sum or difference of several measurements such that \[Q = \left(a+b+...+c\right) - \left(x+y+...+z\right)\]
then 
\[\delta Q = \sqrt{[\left(\delta a\right)^2 + \left(\delta b\right)^2 +...+\left(\delta c\right)^2] + [\left(\delta x\right)^2 + \left(\delta y\right)^2 +...+ \left(\delta z\right)^2]}\]

\subsubsection{General Functions}
Suppose we had two measurements, $\theta = 71^{\circ}$ and $\theta = 73^{\circ}$ with $u = \tan{\theta}$. Without formulas, we might use $\tan{71^{\circ}}$ and $\tan{73^{\circ}}$ to set bounds on the error, subtracting them with the mean. If we generalize this concept to a function $f(x)$, then we have just estimated the error as 
\[ \delta f = f(x+\delta x) - f(x) \]
\[ \delta f = f(x) - f(x - \delta x) \]
But for small $\delta x$, we can approximate the following: 
\[ f(x \pm \delta x) \approx f(x) \pm \frac{df}{dx} \delta x \]
which means that both of these estimates simplify down to
\[ \delta f = \frac{df}{dx} \delta x \]

For a multivariate function $f(x, y, z \cdots)$, the error is given by 
\[ \sigma_f = \sqrt{\left(\frac{\partial f}{\partial x} \sigma_x \right)^2 + \left(\frac{\partial f}{\partial y} \sigma_y \right)^2 + \left(\frac{\partial f}{\partial z} \sigma_z \right)^2 \cdots } \]


\end{document}